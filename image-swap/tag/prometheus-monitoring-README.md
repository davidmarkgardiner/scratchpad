# Prometheus Monitoring for Image Push Jobs

This guide explains how to set up Prometheus monitoring and alerting for the image-push jobs generated by the Kyverno policy.

## Prerequisites

- AKS Kubernetes cluster with Prometheus Operator installed
- Kyverno installed and configured with the image-job-generator policy
- kube-state-metrics deployed in your cluster (required for job metrics)

## Components

1. **ServiceMonitor** - Configures Prometheus to scrape metrics from kube-state-metrics
2. **PrometheusRule** - Defines alerting rules for image-push job failures
3. **Grafana Dashboard** - Provides visualization of job metrics and status

## Installation

### 1. Deploy the ServiceMonitor

```bash
kubectl apply -f prometheus-servicemonitor.yaml
```

This ensures that Prometheus scrapes metrics from kube-state-metrics, which provides information about Kubernetes objects including Jobs.

### 2. Deploy the Alerting Rules

```bash
kubectl apply -f prometheus-image-job-alerts.yaml
```

This creates alerting rules that will trigger when:
- An image-push job fails
- An image-push job is stuck (running for more than 15 minutes)
- An image-push job has reached its backoff limit

### 3. Import the Grafana Dashboard

1. Log in to your Grafana instance
2. Navigate to Dashboards > Import
3. Upload the `grafana-image-jobs-dashboard.json` file or paste its contents
4. Select your Prometheus data source
5. Click Import

## Alert Descriptions

### ImagePushJobFailed

Triggers when any job with a name matching `image-push-job-*` has failed.

**Severity**: Warning  
**Action**: Check the job logs to determine why the image push operation failed.

### ImagePushJobStuck

Triggers when a job has been running for more than 15 minutes without completing.

**Severity**: Warning  
**Action**: Check if the job is stuck in a loop or waiting for a resource.

### ImagePushJobBackoffLimitReached

Triggers when a job has reached its backoff limit (failed multiple times).

**Severity**: Critical  
**Action**: Immediate investigation required as the job has failed repeatedly.

## Customization

### Adjusting Alert Thresholds

Edit the `prometheus-image-job-alerts.yaml` file to adjust:
- The duration before an alert fires (`for` field)
- The conditions that trigger an alert (`expr` field)
- The severity level (`severity` label)

### Modifying the Dashboard

The Grafana dashboard can be customized through the Grafana UI:
- Add additional panels
- Adjust time ranges
- Configure additional alerts

## Troubleshooting

### Verifying Metrics Collection

Check if job metrics are being collected:

```bash
kubectl port-forward svc/prometheus-operated 9090:9090 -n monitoring
```

Then visit http://localhost:9090 and query for:
```
kube_job_status_failed{job_name=~"image-push-job-.*"}
```

### Common Issues

1. **No metrics for jobs**: Ensure kube-state-metrics is running and the ServiceMonitor is correctly configured
2. **Alerts not firing**: Check that PrometheusRule is correctly deployed and Prometheus Operator is functioning
3. **Dashboard shows no data**: Verify the Prometheus data source in Grafana is correctly configured

## Integration with Notification Systems

The alerts can be integrated with various notification systems:

- **Slack**: Configure AlertManager to send notifications to Slack
- **Email**: Set up email notifications through AlertManager
- **PagerDuty**: For critical alerts requiring immediate attention
- **Microsoft Teams**: Using a webhook integration

## Additional Monitoring Considerations

- Consider adding metrics for job duration to track performance trends
- Monitor resource usage of jobs to identify potential resource constraints
- Set up logging integration to correlate alerts with log events 